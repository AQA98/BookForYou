{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "One of the biggest challenges when wanting to read books is finding the right book to read. That is why we made BookForYou. BookForYou is a recommender system that suggests books for the user based on their inputted preferences for author, title, and book category. It uses book reviews from Amazonâ€™s Book database to find the ideal book candidate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification of required data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is [Amazon Book Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews?select=books_data.csv).\n",
    "\n",
    "The dataset consists of two entities, one with book details and the second containing book reviews. Each entity has 10 features, for a combined dataset size of 3.04 GB. As shown below, one book can have many reviews, but a review can only belong to a single book. Books are identified by their titles. From the book details, the title, author, year and category will be used. From the reviews entity, the content of the reviews, book rating, and the helpfulness rating of a given review will be used.\n",
    "\n",
    "For reviews the following features will be used:\n",
    "\n",
    "* Id (the id of the book)\n",
    "* title (Book Title)\n",
    "* user_id (Id of user who rate the book)\n",
    "* review/score (rating from 0 to 5 for the book)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data PreProcessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following imports will be used for data preprocesing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"BookForYou Recommender System\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark  = init_spark()\n",
    "df_ratings = spark.read.csv(\"data\\\\preprocessed\\\\reviews.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Data\n",
    "There may frequently be gaps in data sources, which leaves you with three main possibilities for completing the gaps\n",
    "\n",
    "1. Just keep the missing data points.\n",
    "2. Drop them missing data points (including the entire row)\n",
    "3. Fill them in with some other value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.select(\"Id\", \"Title\", \"User_id\", \"review/score\", \"review/summary\")\n",
    "df_ratings = df_ratings.na.drop(subset=[\"Id\",\"Title\",\"User_id\",\"review/score\",\"review/summary\"])\n",
    "df_ratings = df_ratings.withColumnRenamed(\"Id\", \"book_string\")\n",
    "df_ratings = df_ratings.withColumnRenamed(\"User_id\", \"User_string\")\n",
    "df_ratings = df_ratings.filter(df_ratings[\"review/score\"] <= 5)\n",
    "df_ratings = df_ratings.filter(df_ratings[\"review/score\"] >= 1)\n",
    "#df.describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Content-based recommender system****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------+------------+--------------------+\n",
      "|book_string|               Title|   User_string|review/score|      review/summary|\n",
      "+-----------+--------------------+--------------+------------+--------------------+\n",
      "| 1882931173|Its Only Art If I...| AVCGYZL8FQQTD|         4.0|Nice collection o...|\n",
      "| 0826414346|Dr. Seuss: Americ...|A30TK6U7DNS82R|         5.0|   Really Enjoyed It|\n",
      "| 0826414346|Dr. Seuss: Americ...|A3UH4UZ4RSVO82|         5.0|Essential for eve...|\n",
      "| 0826414346|Dr. Seuss: Americ...|A2MVUWT453QH61|         4.0|Phlip Nel gives s...|\n",
      "| 0826414346|Dr. Seuss: Americ...|A22X4XUPKF66MR|         4.0|Good academic ove...|\n",
      "| 0826414346|Dr. Seuss: Americ...|A2F6NONFUDB6UK|         4.0|One of America's ...|\n",
      "| 0826414346|Dr. Seuss: Americ...|A14OJS0VWMOSWO|         5.0|A memorably excel...|\n",
      "| 0826414346|Dr. Seuss: Americ...|A2RSSXTDZDUSH4|         5.0|Academia At It's ...|\n",
      "| 0826414346|Dr. Seuss: Americ...|A25MD5I2GUIW6W|         5.0|And to think that...|\n",
      "| 0826414346|Dr. Seuss: Americ...|A3VA4XFS5WNJO3|         4.0|Fascinating accou...|\n",
      "| 0829814000|Wonderful Worship...| AZ0IOBU20TBOP|         5.0|Outstanding Resou...|\n",
      "| 0829814000|Wonderful Worship...|A373VVEU6Z9M0N|         5.0|Small Churches CA...|\n",
      "| 0829814000|Wonderful Worship...| AGKGOH65VTRR4|         5.0|Not Just for Past...|\n",
      "| 0829814000|Wonderful Worship...| A3OQWLU31BU1Y|         5.0|Small church past...|\n",
      "| 0595344550|Whispers of the W...|A3Q12RK71N74LB|         1.0|            not good|\n",
      "| 0595344550|Whispers of the W...|A1E9M6APK30ZAU|         4.0|  Here is my opinion|\n",
      "| 0595344550|Whispers of the W...| AUR0VA5H0C66C|         1.0|        Buyer beware|\n",
      "| 0595344550|Whispers of the W...|A1YLDZ3VHR6QPZ|         5.0| Fall on your knee's|\n",
      "| 0595344550|Whispers of the W...| ACO23CG8K8T77|         5.0|      Bravo Veronica|\n",
      "| 0595344550|Whispers of the W...|A1VK81CRRC7MLM|         5.0|           Wonderful|\n",
      "+-----------+--------------------+--------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ratings.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#Map strings to numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create a StringIndexer for the \"User_id\" column\n",
    "indexer = StringIndexer(inputCols=[\"book_string\",\"User_string\",\"review/score\",\"Title\"], outputCols=[\"book_id\",\"User_id\",\"rating\",\"Book_title\"])\n",
    "\n",
    "# Fit the StringIndexer to the DataFrame\n",
    "df_ratings = indexer.fit(df_ratings).transform(df_ratings).drop(\"book_string\",\"User_string\", \"review/score\",\"Title\", \"review/summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "| book_id| User_id|rating|\n",
      "+--------+--------+------+\n",
      "|180201.0|167334.0|   1.0|\n",
      "| 40116.0|    64.0|   0.0|\n",
      "| 40116.0|105599.0|   0.0|\n",
      "| 40116.0|  4472.0|   1.0|\n",
      "| 40116.0| 31627.0|   1.0|\n",
      "| 40116.0|  3581.0|   1.0|\n",
      "| 40116.0|     0.0|   0.0|\n",
      "| 40116.0|637113.0|   0.0|\n",
      "| 40116.0|130558.0|   0.0|\n",
      "| 40116.0|837115.0|   1.0|\n",
      "| 76542.0|999164.0|   0.0|\n",
      "| 76542.0|  6737.0|   0.0|\n",
      "| 76542.0|905770.0|   0.0|\n",
      "| 76542.0|271312.0|   0.0|\n",
      "| 11449.0|272659.0|   3.0|\n",
      "| 11449.0|385682.0|   1.0|\n",
      "| 11449.0|307548.0|   3.0|\n",
      "| 11449.0| 38094.0|   0.0|\n",
      "| 11449.0|885875.0|   0.0|\n",
      "| 11449.0|473482.0|   0.0|\n",
      "+--------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "df_ratings=df_ratings.drop(\"Book_title\")\n",
    "df_ratings.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#Combining the features using VectorAssembler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[180201.0,167334....|\n",
      "|  [40116.0,64.0,0.0]|\n",
      "|[40116.0,105599.0...|\n",
      "|[40116.0,4472.0,1.0]|\n",
      "|[40116.0,31627.0,...|\n",
      "|[40116.0,3581.0,1.0]|\n",
      "|   [40116.0,0.0,0.0]|\n",
      "|[40116.0,637113.0...|\n",
      "|[40116.0,130558.0...|\n",
      "|[40116.0,837115.0...|\n",
      "|[76542.0,999164.0...|\n",
      "|[76542.0,6737.0,0.0]|\n",
      "|[76542.0,905770.0...|\n",
      "|[76542.0,271312.0...|\n",
      "|[11449.0,272659.0...|\n",
      "|[11449.0,385682.0...|\n",
      "|[11449.0,307548.0...|\n",
      "|[11449.0,38094.0,...|\n",
      "|[11449.0,885875.0...|\n",
      "|[11449.0,473482.0...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the dataset\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=df_ratings.columns, outputCol='features')\n",
    "df_ratings = assembler.transform(df_ratings)\n",
    "df_ratings.select(df_ratings[\"features\"]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#Normalizing using standardScalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "df_ratings = scaler.fit(df_ratings).transform(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     scaled_features|\n",
      "+--------------------+\n",
      "|[4.48501061902150...|\n",
      "|[0.99844443700460...|\n",
      "|[0.99844443700460...|\n",
      "|[0.99844443700460...|\n",
      "|[0.99844443700460...|\n",
      "|[0.99844443700460...|\n",
      "|[0.99844443700460...|\n",
      "|[0.99844443700460...|\n",
      "|[0.99844443700460...|\n",
      "|[0.99844443700460...|\n",
      "|[1.90504871116777...|\n",
      "|[1.90504871116777...|\n",
      "|[1.90504871116777...|\n",
      "|[1.90504871116777...|\n",
      "|[0.28495339413864...|\n",
      "|[0.28495339413864...|\n",
      "|[0.28495339413864...|\n",
      "|[0.28495339413864...|\n",
      "|[0.28495339413864...|\n",
      "|[0.28495339413864...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ratings.select(\"scaled_features\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#Creating the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(featuresCol='scaled_features', k=4)\n",
    "kmeans = kmeans.fit(df_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions\n",
    "kmeans = kmeans.transform(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+--------------------+--------------------+----------+\n",
      "| book_id| User_id|rating|            features|     scaled_features|prediction|\n",
      "+--------+--------+------+--------------------+--------------------+----------+\n",
      "|180201.0|167334.0|   1.0|[180201.0,167334....|[4.48501061902150...|         2|\n",
      "| 40116.0|    64.0|   0.0|  [40116.0,64.0,0.0]|[0.99844443700460...|         1|\n",
      "| 40116.0|105599.0|   0.0|[40116.0,105599.0...|[0.99844443700460...|         1|\n",
      "| 40116.0|  4472.0|   1.0|[40116.0,4472.0,1.0]|[0.99844443700460...|         1|\n",
      "| 40116.0| 31627.0|   1.0|[40116.0,31627.0,...|[0.99844443700460...|         1|\n",
      "| 40116.0|  3581.0|   1.0|[40116.0,3581.0,1.0]|[0.99844443700460...|         1|\n",
      "| 40116.0|     0.0|   0.0|   [40116.0,0.0,0.0]|[0.99844443700460...|         1|\n",
      "| 40116.0|637113.0|   0.0|[40116.0,637113.0...|[0.99844443700460...|         0|\n",
      "| 40116.0|130558.0|   0.0|[40116.0,130558.0...|[0.99844443700460...|         1|\n",
      "| 40116.0|837115.0|   1.0|[40116.0,837115.0...|[0.99844443700460...|         0|\n",
      "| 76542.0|999164.0|   0.0|[76542.0,999164.0...|[1.90504871116777...|         0|\n",
      "| 76542.0|  6737.0|   0.0|[76542.0,6737.0,0.0]|[1.90504871116777...|         2|\n",
      "| 76542.0|905770.0|   0.0|[76542.0,905770.0...|[1.90504871116777...|         0|\n",
      "| 76542.0|271312.0|   0.0|[76542.0,271312.0...|[1.90504871116777...|         2|\n",
      "| 11449.0|272659.0|   3.0|[11449.0,272659.0...|[0.28495339413864...|         3|\n",
      "| 11449.0|385682.0|   1.0|[11449.0,385682.0...|[0.28495339413864...|         1|\n",
      "| 11449.0|307548.0|   3.0|[11449.0,307548.0...|[0.28495339413864...|         3|\n",
      "| 11449.0| 38094.0|   0.0|[11449.0,38094.0,...|[0.28495339413864...|         1|\n",
      "| 11449.0|885875.0|   0.0|[11449.0,885875.0...|[0.28495339413864...|         0|\n",
      "| 11449.0|473482.0|   0.0|[11449.0,473482.0...|[0.28495339413864...|         0|\n",
      "+--------+--------+------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans.show(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**# Evaluate the Model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1: Silhouette Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score:  0.35501550777553265\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Evaluate the KMeans model using Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette_score = evaluator.evaluate(kmeans)\n",
    "print(\"Silhouette Score: \", silhouette_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2: Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.34797215776495244\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = kmeans.filter(df_ratings[\"rating\"]==kmeans[\"prediction\"]).count() / df_ratings.count()\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3: F1-Measure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.3284099367863862\n"
     ]
    }
   ],
   "source": [
    "# Calculate the F1 measure\n",
    "# Since KMeans is an unsupervised algorithm, we need to convert it into a supervised problem\n",
    "# by comparing predicted cluster labels with actual labels (if available) to compute F1 measure\n",
    "# Assuming you have actual labels in a column called 'label' in your 'data' DataFrame\n",
    "\n",
    "# Import the required libraries for calculating F1 measure\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Convert cluster labels to integer type\n",
    "f1 = kmeans.withColumn(\"prediction\", kmeans[\"prediction\"].cast(\"double\"))\n",
    "\n",
    "\n",
    "\n",
    "# # Instantiate the MulticlassClassificationEvaluator with 'f1' as the metric\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"rating\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "\n",
    "# # Calculate the F1 measure\n",
    "f1_score = f1_evaluator.evaluate(f1)\n",
    "\n",
    "# # Print the F1 measure\n",
    "print(\"F1 Score: \", f1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4: Recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.800670969992221\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have actual labels in a column called 'label' in your 'data' DataFrame\n",
    "# Calculate the recall\n",
    "# Recall is defined as the ratio of true positives to the sum of true positives and false negatives\n",
    "\n",
    "# Count the number of true positives\n",
    "true_positives = kmeans.where((kmeans.rating == 1) & (kmeans.prediction == 1)).count()\n",
    "\n",
    "# Count the number of false negatives\n",
    "false_negatives = kmeans.where((kmeans.rating == 1) & (kmeans.prediction == 0)).count()\n",
    "\n",
    "# Calculate recall\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "# Print the recall\n",
    "print(\"Recall: \", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
