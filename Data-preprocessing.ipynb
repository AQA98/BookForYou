{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "One of the biggest challenges when wanting to read books is finding the right book to read. That is why we made BookForYou. BookForYou is a recommender system that suggests books for the user based on their inputted preferences for author, title, and book category. It uses book reviews from Amazonâ€™s Book database to find the ideal book candidate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification of required data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is [Amazon Book Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews?select=books_data.csv).\n",
    "\n",
    "The dataset consists of two entities, one with book details and the second containing book reviews. Each entity has 10 features, for a combined dataset size of 3.04 GB. As shown below, one book can have many reviews, but a review can only belong to a single book. Books are identified by their titles. From the book details, the title, author, year and category will be used. From the reviews entity, the content of the reviews, book rating, and the helpfulness rating of a given review will be used.\n",
    "\n",
    "![Entities picture](images\\entities.png)\n",
    "\n",
    "For Book_details the following features will be used:\n",
    "For reviews the following features will be used:\n",
    "\n",
    "# can check relevancy of features using correlation metrics (pearson similarity) of decision tree?\n",
    "* Id (the id of the book)\n",
    "* title (Book Title)\n",
    "* user_id (Id of user who rate the book)\n",
    "* review/score (rating from 0 to 5 for the book)\n",
    "* review/summary (the summary of text review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data PreProcessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following imports will be used for data preprocesing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "# Spark imports\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "# Dask imports\n",
    "import dask.bag as db\n",
    "import dask.dataframe as df  # you can use Dask bags or dataframes\n",
    "from csv import reader\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.tree        # For DecisionTreeClassifier class\n",
    "import sklearn.ensemble    # For RandomForestClassifier class\n",
    "import sklearn.datasets    # For make_circles\n",
    "import sklearn.metrics     # For accuracy_score\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"BookForYou Recommender System\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees for feature importance\n",
    "(USELESS CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # define the categorical features to one-hot encode\n",
    "# categorical_cols = ['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
    "\n",
    "\n",
    "# # create a list of string indexers for each categorical feature\n",
    "# indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_cols]\n",
    "\n",
    "# encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "#                                  outputCols=[col + \"_encoded\" for col in categorical_cols])\n",
    "\n",
    "# # fit the indexers and encoder to the data\n",
    "# indexers_models = [indexer.fit(df) for indexer in indexers]\n",
    "# encoded_df = encoder.fit(\n",
    "#     reduce(lambda data, model: model.transform(data), indexers_models, df)\n",
    "# ).transform(\n",
    "#     reduce(lambda data, model: model.transform(data), indexers_models, df)\n",
    "# )\n",
    "# print(encoded_df.columns)\n",
    "# dt = DecisionTreeClassifier(labelCol=\"review/score_index\", featuresCol='review/summary_encoded', maxDepth=2, maxBins=1000)\n",
    "# model = dt.fit(encoded_df)\n",
    "# print(model.featureImportances)\n",
    "# print(df.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews Table \n",
    "We will read the data from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark  = init_spark()\n",
    "df = spark.read.csv(\"data\\\\preprocessed\\\\reviews.csv\", header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Data\n",
    "There may frequently be gaps in data sources, which leaves you with three main possibilities for completing the gaps\n",
    "\n",
    "1. Just keep the missing data points.\n",
    "2. Drop them missing data points (including the entire row)\n",
    "3. Fill them in with some other value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2420208\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\"Id\", \"Title\", \"User_id\", \"review/score\", \"review/summary\")\n",
    "df.na.drop(subset=[\"Id\",\"Title\",\"User_id\",\"review/score\",\"review/summary\"])\n",
    "df = df.filter(df[\"review/score\"] <= 5)\n",
    "df = df.filter(df[\"review/score\"] >= 1)\n",
    "print(df.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
