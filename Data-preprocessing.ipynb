{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "One of the biggest challenges when wanting to read books is finding the right book to read. That is why we made BookForYou. BookForYou is a recommender system that suggests books for the user based on their inputted preferences for author, title, and book category. It uses book reviews from Amazonâ€™s Book database to find the ideal book candidate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification of required data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is [Amazon Book Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews?select=books_data.csv).\n",
    "\n",
    "The dataset consists of two entities, one with book details and the second containing book reviews. Each entity has 10 features, for a combined dataset size of 3.04 GB. As shown below, one book can have many reviews, but a review can only belong to a single book. Books are identified by their titles. From the book details, the title, author, year and category will be used. From the reviews entity, the content of the reviews, book rating, and the helpfulness rating of a given review will be used.\n",
    "\n",
    "![Entities picture](images\\entities.png)\n",
    "\n",
    "In the `book_details.csv` file, we will only keep some of the features. The following features will be removed\n",
    "-   image\n",
    "-   previewLink\n",
    "-   infoLink\n",
    "\n",
    "For null or empty values for the features being kept, we will\n",
    "-   Remove the entry if `title` is NULL\n",
    "-   Use the `reviews_text` feature if `description` is NULL\n",
    "-   Take the mean of other values in `categories` if it is NULL\n",
    "    -   Only years will be kept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data PreProcessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification of required data\n",
    "In the `book_details.csv` file, we identify which data will be useful to our recommender system.\n",
    "For this entity, we will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# #book_details = dd.read_csv('data/preprocessed/book_details.csv')\n",
    "# book_ratings= dd.read_csv('data/preprocessed/reviews.csv', blocksize=1000)\n",
    "# #book_details.head(10)\n",
    "# book_ratings.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "# Spark imports\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "# Dask imports\n",
    "import dask.bag as db\n",
    "import dask.dataframe as details  # you can use Dask bags or dataframes\n",
    "from csv import reader\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark initialization\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abdul Samad\\Documents\\BookForYou\\Data-preprocessing.ipynb Cell 9\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Samad/Documents/BookForYou/Data-preprocessing.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m col, to_date\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Samad/Documents/BookForYou/Data-preprocessing.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cols \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mauthors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpublisher\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpublishedDate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mratingsCount\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Samad/Documents/BookForYou/Data-preprocessing.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mpreprocessed\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mbook_details.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, usecols \u001b[39m=\u001b[39;49m cols)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Samad/Documents/BookForYou/Data-preprocessing.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m spark  \u001b[39m=\u001b[39m init_spark()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abdul%20Samad/Documents/BookForYou/Data-preprocessing.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Abdul Samad\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, to_date\n",
    "cols = [\"Title\", \"description\", \"authors\", \"publisher\", \"publishedDate\", \"categories\",\"ratingsCount\"]\n",
    "\n",
    "df = pd.read_csv(\"data\\\\preprocessed\\\\book_details.csv\", usecols = cols)\n",
    "spark  = init_spark()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df.fillna('NAN', inplace=True)\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    " \n",
    "# Creating the DataFrame\n",
    "details = spark.createDataFrame(df)\n",
    " \n",
    "# Show the DataFrame\n",
    "\n",
    "details.show()\n",
    "#spark.read.csv(\"data\\\\preprocessed\\\\book_details.csv\", header=True)\n",
    "\n",
    "# #details = details.select()\n",
    "# details.show()\n",
    "#df.head(10)\n",
    "\n",
    "# details = details.filter(details[\"Title\"] != '')\n",
    "# details = details.fillna({\"authors\":\"UNKNOWN\"})\n",
    "# details = details.fillna({\"publisher\":\"UNKNOWN\"})\n",
    "# details = details.fillna({\"publishedDate\":\"UNKNOWN\"})\n",
    "# pattern = \"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"\n",
    "\n",
    "# # Use the PySpark 'to_date' function to convert the column values to dates\n",
    "# # Only select the values that match the defined pattern using the 'rlike' function\n",
    "# dates_df = details.select(to_datetime(col(\"publishedDate\"), \"yyyy-MM-dd\").alias(\"date\")) \\\n",
    "#              .filter(col(\"date_column\").rlike(pattern))\n",
    "\n",
    "# dates_df.head(20)\n",
    "# if description is null, use review text\n",
    "# i = 0\n",
    "# for row in details.collect():\n",
    "#   publish_date = row['publishedDate']\n",
    "#   i= i+1\n",
    "#   try:\n",
    "\n",
    "#     publish_date = datetime.strptime(publish_date, '%Y-%m-%d').date()\n",
    "\n",
    "#   except:\n",
    "#     print(f'row {row}')\n",
    "#     print(\"Bad dates:\", publish_date)\n",
    "\n",
    "#   if(i==7):\n",
    "#     break\n",
    "\n",
    "  # publish_date = datetime.strptime(publish_date, '%Y-%m-%d').date()\n",
    "  # publish_year = publish_date.year\n",
    "  # print(type(publish_date))\n",
    " \n",
    "    # print(details[row])\n",
    "    # break;\n",
    "# details = details.filter(details[\"User_id\"] != '')\n",
    "\n",
    "# details = details.filter(details[\"review/score\"] <= 5)\n",
    "# details = details.filter(details[\"review/score\"] >= 1)\n",
    "# no need to filter null values from review/summary\n",
    "# details.dropna()\n",
    "# details.show()\n",
    "# print(details.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPYY\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, to_date\n",
    "cols = [\"Title\", \"description\", \"authors\", \"publisher\", \"publishedDate\", \"categories\",\"ratingsCount\"]\n",
    "\n",
    "df = pd.read_csv(\"data\\\\preprocessed\\\\book_details.csv\", usecols = cols)\n",
    "spark  = init_spark()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df.fillna('NAN', inplace=True)\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    " \n",
    "# Creating the DataFrame\n",
    "details = spark.createDataFrame(df)\n",
    " \n",
    "# Show the DataFrame\n",
    "\n",
    "details.show()\n",
    "#spark.read.csv(\"data\\\\preprocessed\\\\book_details.csv\", header=True)\n",
    "\n",
    "# #details = details.select()\n",
    "# details.show()\n",
    "#df.head(10)\n",
    "\n",
    "# details = details.filter(details[\"Title\"] != '')\n",
    "# details = details.fillna({\"authors\":\"UNKNOWN\"})\n",
    "# details = details.fillna({\"publisher\":\"UNKNOWN\"})\n",
    "# details = details.fillna({\"publishedDate\":\"UNKNOWN\"})\n",
    "# pattern = \"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"\n",
    "\n",
    "# # Use the PySpark 'to_date' function to convert the column values to dates\n",
    "# # Only select the values that match the defined pattern using the 'rlike' function\n",
    "# dates_df = details.select(to_datetime(col(\"publishedDate\"), \"yyyy-MM-dd\").alias(\"date\")) \\\n",
    "#              .filter(col(\"date_column\").rlike(pattern))\n",
    "\n",
    "# dates_df.head(20)\n",
    "# if description is null, use review text\n",
    "# i = 0\n",
    "# for row in details.collect():\n",
    "#   publish_date = row['publishedDate']\n",
    "#   i= i+1\n",
    "#   try:\n",
    "\n",
    "#     publish_date = datetime.strptime(publish_date, '%Y-%m-%d').date()\n",
    "\n",
    "#   except:\n",
    "#     print(f'row {row}')\n",
    "#     print(\"Bad dates:\", publish_date)\n",
    "\n",
    "#   if(i==7):\n",
    "#     break\n",
    "\n",
    "  # publish_date = datetime.strptime(publish_date, '%Y-%m-%d').date()\n",
    "  # publish_year = publish_date.year\n",
    "  # print(type(publish_date))\n",
    " \n",
    "    # print(details[row])\n",
    "    # break;\n",
    "# details = details.filter(details[\"User_id\"] != '')\n",
    "\n",
    "# details = details.filter(details[\"review/score\"] <= 5)\n",
    "# details = details.filter(details[\"review/score\"] >= 1)\n",
    "# no need to filter null values from review/summary\n",
    "# details.dropna()\n",
    "# details.show()\n",
    "# print(details.count())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
