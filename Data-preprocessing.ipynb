{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "One of the biggest challenges when wanting to read books is finding the right book to read. That is why we made BookForYou. BookForYou is a recommender system that suggests books for the user based on their inputted preferences for author, title, and book category. It uses book reviews from Amazonâ€™s Book database to find the ideal book candidate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification of required data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is [Amazon Book Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews?select=books_data.csv).\n",
    "\n",
    "The dataset consists of two entities, one with book details and the second containing book reviews. Each entity has 10 features, for a combined dataset size of 3.04 GB. As shown below, one book can have many reviews, but a review can only belong to a single book. Books are identified by their titles. From the book details, the title, author, year and category will be used. From the reviews entity, the content of the reviews, book rating, and the helpfulness rating of a given review will be used.\n",
    "\n",
    "![Entities picture](images\\entities.png)\n",
    "\n",
    "In the `book_details.csv` file, we will only keep some of the features. The following features will be removed\n",
    "-   image\n",
    "-   previewLink\n",
    "-   infoLink\n",
    "\n",
    "For null or empty values for the features being kept, we will\n",
    "-   Remove the entry if `title` is NULL\n",
    "-   Use the `reviews_text` feature if `description` is NULL\n",
    "-   Take the mean of other values in `categories` if it is NULL\n",
    "    -   Only years will be kept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data PreProcessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification of required data\n",
    "In the `book_details.csv` file, we identify which data will be useful to our recommender system.\n",
    "For this entity, we will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# #book_details = dd.read_csv('data/preprocessed/book_details.csv')\n",
    "# book_ratings= dd.read_csv('data/preprocessed/reviews.csv', blocksize=1000)\n",
    "# #book_details.head(10)\n",
    "# book_ratings.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "# Spark imports\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "# Dask imports\n",
    "import dask.bag as db\n",
    "import dask.dataframe as details  # you can use Dask bags or dataframes\n",
    "from csv import reader\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark initialization\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, to_date\n",
    "cols = [\"Title\", \"description\", \"authors\", \"publisher\", \"publishedDate\", \"categories\",\"ratingsCount\"]\n",
    "\n",
    "df = pd.read_csv(\"data\\\\preprocessed\\\\book_details.csv\", usecols = cols)\n",
    "spark  = init_spark()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df.fillna('NAN', inplace=True)\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    " \n",
    "# Creating the DataFrame\n",
    "details = spark.createDataFrame(df)\n",
    " \n",
    "# Show the DataFrame\n",
    "\n",
    "details.show()\n",
    "#spark.read.csv(\"data\\\\preprocessed\\\\book_details.csv\", header=True)\n",
    "\n",
    "# #details = details.select()\n",
    "# details.show()\n",
    "#df.head(10)\n",
    "\n",
    "# details = details.filter(details[\"Title\"] != '')\n",
    "# details = details.fillna({\"authors\":\"UNKNOWN\"})\n",
    "# details = details.fillna({\"publisher\":\"UNKNOWN\"})\n",
    "# details = details.fillna({\"publishedDate\":\"UNKNOWN\"})\n",
    "# pattern = \"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"\n",
    "\n",
    "# # Use the PySpark 'to_date' function to convert the column values to dates\n",
    "# # Only select the values that match the defined pattern using the 'rlike' function\n",
    "# dates_df = details.select(to_datetime(col(\"publishedDate\"), \"yyyy-MM-dd\").alias(\"date\")) \\\n",
    "#              .filter(col(\"date_column\").rlike(pattern))\n",
    "\n",
    "# dates_df.head(20)\n",
    "# if description is null, use review text\n",
    "# i = 0\n",
    "# for row in details.collect():\n",
    "#   publish_date = row['publishedDate']\n",
    "#   i= i+1\n",
    "#   try:\n",
    "\n",
    "#     publish_date = datetime.strptime(publish_date, '%Y-%m-%d').date()\n",
    "\n",
    "#   except:\n",
    "#     print(f'row {row}')\n",
    "#     print(\"Bad dates:\", publish_date)\n",
    "\n",
    "#   if(i==7):\n",
    "#     break\n",
    "\n",
    "  # publish_date = datetime.strptime(publish_date, '%Y-%m-%d').date()\n",
    "  # publish_year = publish_date.year\n",
    "  # print(type(publish_date))\n",
    " \n",
    "    # print(details[row])\n",
    "    # break;\n",
    "# details = details.filter(details[\"User_id\"] != '')\n",
    "\n",
    "# details = details.filter(details[\"review/score\"] <= 5)\n",
    "# details = details.filter(details[\"review/score\"] >= 1)\n",
    "# no need to filter null values from review/summary\n",
    "# details.dropna()\n",
    "# details.show()\n",
    "# print(details.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPYY\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, to_date\n",
    "cols = [\"Title\", \"description\", \"authors\", \"publisher\", \"publishedDate\", \"categories\",\"ratingsCount\"]\n",
    "\n",
    "df = pd.read_csv(\"data\\\\preprocessed\\\\book_details.csv\", usecols = cols)\n",
    "spark  = init_spark()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df.fillna('NAN', inplace=True)\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    " \n",
    "# Creating the DataFrame\n",
    "details = spark.createDataFrame(df)\n",
    " \n",
    "# Show the DataFrame\n",
    "\n",
    "details.show()\n",
    "#spark.read.csv(\"data\\\\preprocessed\\\\book_details.csv\", header=True)\n",
    "\n",
    "# #details = details.select()\n",
    "# details.show()\n",
    "#df.head(10)\n",
    "\n",
    "# details = details.filter(details[\"Title\"] != '')\n",
    "# details = details.fillna({\"authors\":\"UNKNOWN\"})\n",
    "# details = details.fillna({\"publisher\":\"UNKNOWN\"})\n",
    "# details = details.fillna({\"publishedDate\":\"UNKNOWN\"})\n",
    "# pattern = \"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\"\n",
    "\n",
    "# # Use the PySpark 'to_date' function to convert the column values to dates\n",
    "# # Only select the values that match the defined pattern using the 'rlike' function\n",
    "# dates_df = details.select(to_datetime(col(\"publishedDate\"), \"yyyy-MM-dd\").alias(\"date\")) \\\n",
    "#              .filter(col(\"date_column\").rlike(pattern))\n",
    "\n",
    "# dates_df.head(20)\n",
    "# if description is null, use review text\n",
    "# i = 0\n",
    "# for row in details.collect():\n",
    "#   publish_date = row['publishedDate']\n",
    "#   i= i+1\n",
    "#   try:\n",
    "\n",
    "#     publish_date = datetime.strptime(publish_date, '%Y-%m-%d').date()\n",
    "\n",
    "#   except:\n",
    "#     print(f'row {row}')\n",
    "#     print(\"Bad dates:\", publish_date)\n",
    "\n",
    "#   if(i==7):\n",
    "#     break\n",
    "\n",
    "  # publish_date = datetime.strptime(publish_date, '%Y-%m-%d').date()\n",
    "  # publish_year = publish_date.year\n",
    "  # print(type(publish_date))\n",
    " \n",
    "    # print(details[row])\n",
    "    # break;\n",
    "# details = details.filter(details[\"User_id\"] != '')\n",
    "\n",
    "# details = details.filter(details[\"review/score\"] <= 5)\n",
    "# details = details.filter(details[\"review/score\"] >= 1)\n",
    "# no need to filter null values from review/summary\n",
    "# details.dropna()\n",
    "# details.show()\n",
    "# print(details.count())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
